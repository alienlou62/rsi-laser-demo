#include "camera_grpc_server.h"
#include "rttaskglobals.h"
#include "camera_helpers.h"
#include "image_processing.h"

#include <grpcpp/grpcpp.h>
#include <opencv2/opencv.hpp>

// Include generated protobuf files (these will be generated by CMake)
#include "camera_streaming.grpc.pb.h"

#include <iostream>
#include <cstring>

// Platform-specific includes
#ifdef _WIN32
    #include <windows.h>
#else
    #include <sys/mman.h>
    #include <sys/stat.h>
    #include <fcntl.h>
    #include <unistd.h>
    #include <errno.h>
#endif

using namespace RSI::RapidCode::RealTimeTasks;

namespace CameraGrpcServer {

// gRPC service implementation
class CameraStreamServiceImpl final : public rsi::camera::CameraStreamService::Service {
public:
    explicit CameraStreamServiceImpl(RTTaskManager* taskManager) 
        : m_taskManager(taskManager) {}

    grpc::Status StreamCameraFrames(
        grpc::ServerContext* context,
        const rsi::camera::StreamRequest* request,
        grpc::ServerWriter<rsi::camera::CameraFrame>* writer) override {
        
        // Mark that we have active streaming clients
        ImageBuffer::GetInstance().SetActiveClients(true);
        
        uint32_t lastSequenceNumber = 0;
        auto maxFps = request->max_fps();
        auto frameInterval = maxFps > 0 ? 
            std::chrono::milliseconds(1000 / maxFps) : 
            std::chrono::milliseconds(33); // Default ~30 FPS
        
        std::cout << "Starting camera stream with max FPS: " << maxFps << std::endl;
        
        while (!context->IsCancelled()) {
            try {
                rsi::camera::CameraFrame frame;
                if (BuildCameraFrame(frame, request->format(), request->compression_quality(), lastSequenceNumber)) {
                    if (!writer->Write(frame)) {
                        std::cout << "Client disconnected from stream" << std::endl;
                        break; // Client disconnected
                    }
                    lastSequenceNumber = frame.frame_number();
                }
                
                std::this_thread::sleep_for(frameInterval);
            }
            catch (const std::exception& e) {
                std::cerr << "Error in streaming: " << e.what() << std::endl;
                break;
            }
        }
        
        // Mark that streaming client disconnected
        ImageBuffer::GetInstance().SetActiveClients(false);
        
        std::cout << "Camera stream ended" << std::endl;
        return grpc::Status::OK;
    }

    grpc::Status GetLatestFrame(
        grpc::ServerContext* context,
        const rsi::camera::FrameRequest* request,
        rsi::camera::CameraFrame* response) override {
        
        try {
            uint32_t dummy = 0;
            if (BuildCameraFrame(*response, request->format(), request->compression_quality(), dummy)) {
                return grpc::Status::OK;
            } else {
                return grpc::Status(grpc::StatusCode::UNAVAILABLE, "No frame available");
            }
        }
        catch (const std::exception& e) {
            return grpc::Status(grpc::StatusCode::INTERNAL, e.what());
        }
    }

private:
    RTTaskManager* m_taskManager;

    bool BuildCameraFrame(rsi::camera::CameraFrame& frame, 
                         rsi::camera::ImageFormat format, 
                         int32_t quality,
                         uint32_t& lastSequenceNumber) {
        
        if (!m_taskManager) {
            return false;
        }

        try {
            // Get global values from RT task
            auto newImageAvailable = m_taskManager->GlobalValueGet("newImageAvailable");
            auto frameTimestamp = m_taskManager->GlobalValueGet("frameTimestamp");
            auto imageWidth = m_taskManager->GlobalValueGet("imageWidth");
            auto imageHeight = m_taskManager->GlobalValueGet("imageHeight");
            auto ballDetected = m_taskManager->GlobalValueGet("ballDetected");
            auto ballCenterX = m_taskManager->GlobalValueGet("ballCenterX");
            auto ballCenterY = m_taskManager->GlobalValueGet("ballCenterY");
            auto ballRadius = m_taskManager->GlobalValueGet("ballRadius");
            auto frameGrabFailures = m_taskManager->GlobalValueGet("frameGrabFailures");
            auto ballDetectionFailures = m_taskManager->GlobalValueGet("ballDetectionFailures");

            // Check if new image is available
            if (!newImageAvailable.Bool) {
                return false;
            }

            // Get image data from shared buffer
            ImageBuffer& imgBuffer = ImageBuffer::GetInstance();
            if (!imgBuffer.IsInitialized()) {
                return false;
            }

            // Calculate expected image size for YUYV format
            size_t expectedSize = imageWidth.Int32 * imageHeight.Int32 * 2; // 2 bytes per pixel for YUYV
            std::vector<uint8_t> imageData(expectedSize);
            size_t actualSize;
            uint32_t sequenceNumber;

            if (!imgBuffer.GetLatestImage(imageData.data(), actualSize, sequenceNumber)) {
                return false;
            }

            // Only send if this is a new frame
            if (sequenceNumber == lastSequenceNumber) {
                return false;
            }

            // Set frame metadata
            frame.set_timestamp_us(frameTimestamp.Int64);
            frame.set_frame_number(sequenceNumber);
            frame.set_width(imageWidth.Int32);
            frame.set_height(imageHeight.Int32);

            // Convert and set image data based on requested format
            std::vector<uint8_t> outputData;
            if (ConvertImageFormat(imageData, actualSize, imageWidth.Int32, imageHeight.Int32, 
                                 format, quality, outputData)) {
                frame.set_image_data(outputData.data(), outputData.size());
                frame.set_format(format);
            } else {
                // Fallback to raw YUYV
                frame.set_image_data(imageData.data(), actualSize);
                frame.set_format(rsi::camera::FORMAT_YUYV);
            }

            // Set ball detection data
            auto* ballDetection = frame.mutable_ball_detection();
            ballDetection->set_detected(ballDetected.Bool);
            ballDetection->set_center_x(ballCenterX.Double);
            ballDetection->set_center_y(ballCenterY.Double);
            ballDetection->set_radius(ballRadius.Double);
            ballDetection->set_confidence(ballDetected.Bool ? 85.0 : 0.0); // Simple confidence

            // Set processing stats
            auto* stats = frame.mutable_stats();
            stats->set_grab_failures(frameGrabFailures.Int32);
            stats->set_detection_failures(ballDetectionFailures.Int32);
            // TODO: Add actual timing measurements
            stats->set_grab_time_ms(1.0);
            stats->set_process_time_ms(15.0);

            return true;
        }
        catch (const std::exception& e) {
            std::cerr << "Error building camera frame: " << e.what() << std::endl;
            return false;
        }
    }

    bool ConvertImageFormat(const std::vector<uint8_t>& inputData, size_t inputSize,
                           int width, int height,
                           rsi::camera::ImageFormat format, int32_t quality,
                           std::vector<uint8_t>& outputData) {
        
        try {
            // Create OpenCV Mat from YUYV data
            cv::Mat yuyvMat(height, width, CV_8UC2, (void*)inputData.data());
            
            switch (format) {
                case rsi::camera::FORMAT_YUYV:
                    // Raw YUYV - just copy
                    outputData.assign(inputData.begin(), inputData.begin() + inputSize);
                    return true;
                    
                case rsi::camera::FORMAT_RGB: {
                    // Convert YUYV to RGB
                    cv::Mat rgbMat;
                    cv::cvtColor(yuyvMat, rgbMat, cv::COLOR_YUV2RGB_YUYV);
                    size_t rgbSize = rgbMat.total() * rgbMat.elemSize();
                    outputData.assign(rgbMat.data, rgbMat.data + rgbSize);
                    return true;
                }
                
                case rsi::camera::FORMAT_JPEG: {
                    // Convert to RGB first, then encode as JPEG
                    cv::Mat rgbMat;
                    cv::cvtColor(yuyvMat, rgbMat, cv::COLOR_YUV2RGB_YUYV);
                    
                    std::vector<int> compression_params;
                    compression_params.push_back(cv::IMWRITE_JPEG_QUALITY);
                    compression_params.push_back(std::max(0, std::min(100, quality)));
                    
                    if (cv::imencode(".jpg", rgbMat, outputData, compression_params)) {
                        return true;
                    }
                    break;
                }
                
                case rsi::camera::FORMAT_GRAYSCALE: {
                    // Extract grayscale from YUYV (just the Y channel)
                    cv::Mat grayMat;
                    cv::cvtColor(yuyvMat, grayMat, cv::COLOR_YUV2GRAY_YUYV);
                    size_t graySize = grayMat.total() * grayMat.elemSize();
                    outputData.assign(grayMat.data, grayMat.data + graySize);
                    return true;
                }
                
                default:
                    break;
            }
        }
        catch (const std::exception& e) {
            std::cerr << "Error converting image format: " << e.what() << std::endl;
        }
        
        return false;
    }
};

// ImageBuffer implementation
ImageBuffer& ImageBuffer::GetInstance() {
    static ImageBuffer instance;
    return instance;
}

bool ImageBuffer::Initialize(size_t bufferSize) {
    std::lock_guard<std::mutex> lock(m_mutex);
    
    if (m_buffer) {
        Cleanup();
    }
    
    m_bufferSize = bufferSize;
    
#ifdef _WIN32
    // Create shared memory on Windows
    m_memoryHandle = CreateFileMappingA(
        INVALID_HANDLE_VALUE,
        nullptr,
        PAGE_READWRITE,
        0,
        static_cast<DWORD>(bufferSize),
        "RSICameraImageBuffer"
    );
    
    if (!m_memoryHandle) {
        std::cerr << "Failed to create shared memory: " << GetLastError() << std::endl;
        return false;
    }
    
    m_buffer = MapViewOfFile(m_memoryHandle, FILE_MAP_ALL_ACCESS, 0, 0, bufferSize);
    if (!m_buffer) {
        std::cerr << "Failed to map shared memory: " << GetLastError() << std::endl;
        CloseHandle(m_memoryHandle);
        m_memoryHandle = nullptr;
        return false;
    }
#else
    // Create shared memory on Linux/Unix
    const char* shmName = "/RSICameraImageBuffer";
    
    // Create/open shared memory object
    int fd = shm_open(shmName, O_CREAT | O_RDWR, 0666);
    if (fd == -1) {
        std::cerr << "Failed to create shared memory: " << strerror(errno) << std::endl;
        return false;
    }
    
    // Set the size of the shared memory object
    if (ftruncate(fd, bufferSize) == -1) {
        std::cerr << "Failed to set shared memory size: " << strerror(errno) << std::endl;
        close(fd);
        shm_unlink(shmName);
        return false;
    }
    
    // Map the shared memory object
    m_buffer = mmap(nullptr, bufferSize, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
    if (m_buffer == MAP_FAILED) {
        std::cerr << "Failed to map shared memory: " << strerror(errno) << std::endl;
        close(fd);
        shm_unlink(shmName);
        m_buffer = nullptr;
        return false;
    }
    
    // Store the file descriptor for cleanup
    m_memoryHandle = reinterpret_cast<void*>(static_cast<intptr_t>(fd));
#endif
    
    m_currentSize = 0;
    m_sequenceNumber = 0;
    
    std::cout << "Initialized image buffer with size: " << bufferSize << " bytes" << std::endl;
    return true;
}

void ImageBuffer::Cleanup() {
    std::lock_guard<std::mutex> lock(m_mutex);
    
    if (m_buffer) {
#ifdef _WIN32
        UnmapViewOfFile(m_buffer);
#else
        munmap(m_buffer, m_bufferSize);
#endif
        m_buffer = nullptr;
    }
    
    if (m_memoryHandle) {
#ifdef _WIN32
        CloseHandle(m_memoryHandle);
#else
        int fd = static_cast<int>(reinterpret_cast<intptr_t>(m_memoryHandle));
        close(fd);
        // Unlink the shared memory object to clean it up
        shm_unlink("/RSICameraImageBuffer");
#endif
        m_memoryHandle = nullptr;
    }
    
    m_bufferSize = 0;
    m_currentSize = 0;
}

bool ImageBuffer::StoreImage(const void* imageData, size_t size, uint32_t sequenceNumber) {
    std::lock_guard<std::mutex> lock(m_mutex);
    
    if (!m_buffer || size > m_bufferSize) {
        return false;
    }
    
    std::memcpy(m_buffer, imageData, size);
    m_currentSize = size;
    m_sequenceNumber = sequenceNumber;
    
    return true;
}

bool ImageBuffer::TryStoreImageNonBlocking(const void* imageData, size_t size, uint32_t sequenceNumber) {
    // Try to acquire lock without blocking - critical for real-time safety
    std::unique_lock<std::mutex> lock(m_mutex, std::try_to_lock);
    
    if (!lock.owns_lock()) {
        // Could not acquire lock immediately - skip to maintain timing
        return false;
    }
    
    if (!m_buffer || size > m_bufferSize) {
        return false;
    }
    
    std::memcpy(m_buffer, imageData, size);
    m_currentSize = size;
    m_sequenceNumber = sequenceNumber;
    
    return true;
}

bool ImageBuffer::GetLatestImage(void* buffer, size_t& size, uint32_t& sequenceNumber) const {
    std::lock_guard<std::mutex> lock(m_mutex);
    
    if (!m_buffer || m_currentSize == 0) {
        return false;
    }
    
    std::memcpy(buffer, m_buffer, m_currentSize);
    size = m_currentSize;
    sequenceNumber = m_sequenceNumber;
    
    return true;
}

// CameraStreamServer implementation
CameraStreamServer::CameraStreamServer() = default;

CameraStreamServer::~CameraStreamServer() {
    Stop();
}

bool CameraStreamServer::Start(const std::string& server_address, RTTaskManager* taskManager) {
    if (m_isRunning.load()) {
        return false;
    }
    
    m_taskManager = taskManager;
    
    try {
        // Initialize image buffer (1MB should be enough for 640x480 images)
        ImageBuffer& imgBuffer = ImageBuffer::GetInstance();
        if (!imgBuffer.Initialize(1024 * 1024)) {
            std::cerr << "Failed to initialize image buffer" << std::endl;
            return false;
        }
        
        // Create gRPC service
        auto service = std::make_unique<CameraStreamServiceImpl>(taskManager);
        
        grpc::ServerBuilder builder;
        builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());
        builder.RegisterService(service.get());
        
        // Increase message size limits for image data
        builder.SetMaxReceiveMessageSize(4 * 1024 * 1024); // 4MB
        builder.SetMaxSendMessageSize(4 * 1024 * 1024);    // 4MB
        
        m_server = builder.BuildAndStart();
        
        if (!m_server) {
            std::cerr << "Failed to start gRPC server" << std::endl;
            return false;
        }
        
        m_isRunning.store(true);
        std::cout << "Camera gRPC server listening on " << server_address << std::endl;
        
        // Keep service alive (this will be called from a separate thread)
        service.release(); // Transfer ownership to server
        
        return true;
    }
    catch (const std::exception& e) {
        std::cerr << "Error starting gRPC server: " << e.what() << std::endl;
        return false;
    }
}

void CameraStreamServer::Stop() {
    if (m_server && m_isRunning.load()) {
        m_server->Shutdown();
        m_server->Wait();
        m_server.reset();
        m_isRunning.store(false);
        
        // Cleanup image buffer
        ImageBuffer::GetInstance().Cleanup();
        
        std::cout << "Camera gRPC server stopped" << std::endl;
    }
}

} // namespace CameraGrpcServer
